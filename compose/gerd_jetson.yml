services:
  llama-cpp:
    container_name: llama-cpp
    image: dustynv/llama_cpp:${LLAMA_CPP_VERSION}
    ports:
      - "8000:8000"
    entrypoint:
      - "/bin/sh"
      - "-c"
      - "huggingface-cli download ${HF_REPO} ${GGUF_FILE} --cache-dir /root/.cache/llama.cpp --local-dir /root/.cache/llama.cpp;\
         llama-server -m /root/.cache/llama.cpp/${GGUF_FILE} --port 8000 --host 0.0.0.0 ${MODEL_ARGS}"
    volumes:
      - cto-llm-storage:/root/.cache/llama.cpp
    runtime: nvidia
  gerd:
    container_name: gerd
    image: ghcr.io/caretech-owl/gerd-jetson:${GERD_VERSION}
    ports:
      - "7860:7860"
      - ${GRADIO_SERVER_PORT}:${GRADIO_SERVER_PORT}
    environment:
      GRADIO_SERVER_PORT: "${GRADIO_SERVER_PORT}"
      HF_HOME: "/workspace/hf"
      gerd_gen_model__endpoint__type: "llama.cpp"
      gerd_gen_model__endpoint__url: "http://llama-cpp:8000"
      gerd_lora_output_dir: "/workspace/loras/lora"
      gerd_qa_model__endpoint__type: "llama.cpp"
      gerd_qa_model__endpoint__url: "http://llama-cpp:8000"
    volumes:
      - cto-hf-home:/workspace/hf
      - cto-loras-storage:/workspace/loras
    runtime: nvidia

networks:
  default:
    name: caretech-docker

volumes:
  cto-llm-storage:
    driver: local
  cto-hf-home:
    driver: local
  cto-loras-storage:
    driver: local