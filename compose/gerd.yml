services:
  llama-cpp:
    container_name: llama-cpp
    image: ghcr.io/ggerganov/llama.cpp:${LLAMA_CPP_VERSION}
    ports:
      - "8000:8000"
    entrypoint:
      - "/bin/sh"
      - "-c"
      - "huggingface-cli download ${HF_REPO} ${GGUF_FILE} --cache-dir /models --local-dir /models;\
         ./llama-server -m /models/${GGUF_FILE} --port 8000 --host 0.0.0.0 ${MODEL_ARGS}"
    volumes:
      - cto-llm-storage:/models

  gerd:
    container_name: gerd
    image: ghcr.io/caretech-owl/gerd:${GERD_VERSION}
    ports:
      - "7860:7860"
    environment:
      gerd_qa_model__endpoint__url: "http://llama-cpp:8000"
      gerd_qa_model__endpoint__type: "llama.cpp"
      HF_HOME: "/workspace/hf"
    volumes:
      - cto-hf-home:/workspace/hf

networks:
  default:
    name: caretech-docker

volumes:
  cto-llm-storage:
    driver: local
  cto-hf-home:
    driver: local
